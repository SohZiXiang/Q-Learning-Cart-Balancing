# Q-Learning-Cart-Master
SC3000 lab to balance cartpole 

## Overview
This repository showcases my work in developing a **Reinforcement Learning (RL) agent** to balance a CartPole using **Q-Learning**, as part of my university lab for **SC3000 Artificial Intelligence**. This project demonstrates concepts of RL, including Q-Learning, exploration vs. exploitation strategies, and reward shaping to encourage effective pole balancing.

### Contributions by Soh Zi Xiang (U2321798D)
- **Environment Setup:** Configured the CartPole environment using OpenAI Gym for experimentation.
- **Q-Learning Model Development:** Implemented and fine-tuned the Q-Learning algorithm.
- **Modifications and Enhancements:** Improved the learning process by modifying code for better convergence and stability:
  - Reduced episodes and increased learning rate.
  - Enhanced state discretization with `np.clip`.
  - Adjusted environment handling for compatibility with newer versions of OpenAI Gym.
  - Introduced reward shaping to emphasize pole balancing.

## Key Concepts
### Q-Learning Algorithm
The Q-Learning model follows the update rule for estimating Q-values, which represent the expected future rewards for state-action pairs:
$$ Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \big] $$

- **Exploration vs. Exploitation**: 
  - Uses an Îµ-greedy strategy where the epsilon value (\( \epsilon \)) decreases over time to transition from exploration to exploitation.
- **Reward Shaping**:
  - Includes a bonus reward based on the pole's angle to encourage stability.

### Reward Structure
- **Standard Reward**:
  - +1 for each time step the pole stays balanced.
- **Balance Reward**:
  - Additional reward based on pole angle, encouraging the agent to keep the pole closer to vertical.

## Tasks and Results
### Task 1: Train the RL Agent
- **Training Episodes**:
  - The agent was trained for 20,000 episodes.
- **Learning Strategy**:
  - Early episodes focused on exploration to gather diverse experiences.
  - Later episodes relied on exploitation to optimize decision-making.

### Task 2: Evaluate Performance
- **Cumulative Rewards**:
  - The trained model was run for 100 episodes, with cumulative rewards recorded for each episode.
- **Visualization**:
  - A line graph was plotted to display cumulative rewards, demonstrating the effectiveness of the learned Q-values.

### Task 3: Render Episode
- **Rendered Episode**:
  - One episode was rendered and saved as a video to showcase the agent's performance in balancing the pole.
- **Environment Adjustments**:
  - Environment setup was modified for video recording and rendering.

## References
- **CartPole Basics**: Inspired by concepts from the lectures and adapted from [Using Q-Learning for OpenAI's CartPole](https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df).
